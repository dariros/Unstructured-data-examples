{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ek7mo6yenuoqvu7p6qhy",
   "authorId": "48480042761",
   "authorName": "DARIA",
   "authorEmail": "daria.rostovtseva@snowflake.com",
   "sessionId": "f649f300-bf81-457a-8cec-9680c79bd7d8",
   "lastEditTime": 1761677525211
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a6afb4-14b6-435c-95fe-71bba15c14ed",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "# üìÅ Continuous Pipeline for Document Processing\n\n### This notebook provides an end-to-end example of setting up a continuous processing pipeline for documents for the purpose of preparing them for natural language analytics via Cortex Agents in interfaces such as Snowflake Intelligence.\n\n## Step 1: Create a Stage with Directory Table Enabled üèóÔ∏è\n\nCreate a Snowflake stage with directory table functionality to automatically track file uploads and changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61b5fe-2275-446c-8b05-4cbe1596801f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "language": "sql",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "-- üîß Setup: Configure database context and role\n",
    "-- Adjust these values to match your Snowflake environment\n",
    "USE ROLE SYSADMIN;\n",
    "USE SCHEMA ADVANCED_ANALYTICS.UNSTRUCTURED;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "-- üìÅ Create a stage with directory table and auto-refresh enabled\n",
    "-- This allows automatic tracking of file uploads and changes\n",
    "-- Note: If the stage already exists, use ALTER STAGE to enable these features\n",
    "CREATE OR REPLACE STAGE pdfs_stage\n",
    "  ENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE')\n",
    "  DIRECTORY = ( ENABLE = TRUE \n",
    "                AUTO_REFRESH = TRUE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53c4a6-cdb5-4c2a-8ac3-ea5b3915da3f",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "## Step 2: Create a Stream on the Directory Table üåä\n",
    "\n",
    "Set up a stream to capture changes in the directory table, enabling real-time processing of newly uploaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c46e4a-5e5c-453b-bb3d-1799008401cb",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STREAM pdfs_stream ON STAGE pdfs_stage;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e52d6-7049-4480-8ab4-b70f470fc288",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## Step 3: Create Tables for Document Processing üìä\n",
    "\n",
    "Create the necessary tables to store parsed document content and text chunks for efficient retrieval and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28dcee-ee06-4ab5-a561-7d934f8cf12b",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "-- üìä Table for storing parsed document content from AI_PARSE_DOCUMENT\n",
    "CREATE OR REPLACE TABLE parsed_documents (\n",
    "  file_name VARCHAR,\n",
    "  parsed_content VARIANT,\n",
    "  processed_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- üîç Table for chunked text optimized for search and retrieval\n",
    "CREATE OR REPLACE TABLE document_chunks (\n",
    "  file_name VARCHAR,\n",
    "  chunk_id INTEGER,\n",
    "  chunk_text STRING,\n",
    "  processed_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b62f80-01db-44dc-a077-6d209db44cdd",
   "metadata": {
    "collapsed": false,
    "name": "cell28"
   },
   "source": [
    "## Step 3.1: Create Stream on Parsed Documents Table üìà\n",
    "\n",
    "Create a stream on the parsed documents table to monitor for new entries and trigger the chunking process automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bed70-0ef9-4c91-862f-5b3732da9040",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STREAM parsed_documents_stream ON TABLE parsed_documents;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729c5b0-52c6-4e5d-9aca-d8f7e8e9ffa6",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "## Step 4: Create a Task for Document Parsing ü§ñ\n",
    "\n",
    "Set up an automated task that monitors the stream and parses new PDF documents using Snowflake's AI_PARSE_DOCUMENT function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630110b4-c33b-4d76-bd0f-a77754907196",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TASK parse_new_documents\n",
    "  SCHEDULE = '1 minute'\n",
    "  COMMENT = 'Parse new PDF files using AI_PARSE_DOCUMENT'\n",
    "  WHEN\n",
    "  SYSTEM$STREAM_HAS_DATA('pdfs_stream')\n",
    "  AS\n",
    "  INSERT INTO parsed_documents (file_name, parsed_content)\n",
    "  SELECT \n",
    "      relative_path as file_name,\n",
    "      AI_PARSE_DOCUMENT(TO_FILE('@pdfs_stage', relative_path)) as parsed_content\n",
    "  FROM pdfs_stream\n",
    "  WHERE METADATA$ACTION='INSERT';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1451a03-be02-4277-b479-e76f3c2d01f4",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "## Step 5: Create a Task for Text Chunking ‚úÇÔ∏è\n",
    "\n",
    "Create a downstream task that processes parsed documents into smaller, searchable chunks using Snowflake's recursive text splitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235446f1-f7c2-453e-83ce-bd379624c7fc",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TASK chunk_parsed_documents\n",
    "  COMMENT = 'Chunk parsed documents using SPLIT_TEXT_RECURSIVE_CHARACTER'\n",
    "  AFTER parse_new_documents\n",
    "  WHEN\n",
    "  SYSTEM$STREAM_HAS_DATA('parsed_documents_stream')\n",
    "  AS\n",
    "   insert into document_chunks (file_name, chunk_id, chunk_text)\n",
    "\n",
    "    select file_name, \n",
    "            c.INDEX::INTEGER as chunk_id,\n",
    "            c.value::TEXT as chunk_text                \n",
    "    from \n",
    "        parsed_documents_stream,\n",
    "        LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
    "              PARSED_CONTENT,\n",
    "              'markdown',\n",
    "              2000,\n",
    "              300,\n",
    "              ['\\n\\n', '\\n', ' ', '']\n",
    "           )) c;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d48166-e107-4dcc-b827-25ae04486fc3",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "## Step 6: Enable and Test the Pipeline üöÄ\n",
    "\n",
    "Activate the tasks and test the complete pipeline with sample PDF files to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ccbde-01b5-4da0-86b3-83a016641334",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "-- üöÄ Enable all tasks in the pipeline using hierarchical activation\n",
    "-- This will enable the main task and all dependent tasks automatically\n",
    "SELECT SYSTEM$TASK_DEPENDENTS_ENABLE('ADVANCED_ANALYTICS.UNSTRUCTURED.PARSE_NEW_DOCUMENTS');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c36442-f181-41f6-b372-fb586b2a351b",
   "metadata": {
    "collapsed": false,
    "name": "cell20"
   },
   "source": "## Step 6.1: Upload Files and Monitor Stream üìÇ\n\nUpload some PDF files into your stage, then check the stream. It should show new files that are ready to be processed by the pipeline.\n\nIn a production situation, you will set up a process to ingest the files into the stage. For example, you can use an Openflow connector to connect to Sharepoint, or use Snowflake CLI to upload files from a local file system."
  },
  {
   "cell_type": "markdown",
   "id": "47f06f97-192b-4af5-9e49-f01513256bc3",
   "metadata": {
    "name": "cell13"
   },
   "source": [
    "## Step 6.2: Pipeline Verification and Monitoring üîç\n",
    "\n",
    "Use the following queries to monitor and verify that your document processing pipeline is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c8a11-2843-4d67-859d-42db32c38ef1",
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "-- üëÄ Monitor the stream for newly uploaded files\n",
    "-- This should show files that have been uploaded to the stage\n",
    "SELECT * FROM pdfs_stream;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb905b9-6074-4e79-8b37-be6f8aa71e75",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "-- üìÑ Review the parsed document content\n",
    "-- Check that AI_PARSE_DOCUMENT successfully extracted text from PDFs\n",
    "SELECT file_name, parsed_content:content::STRING as extracted_text \n",
    "FROM parsed_documents;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149edd5-cd1c-4f8c-ac41-358b67af649d",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "-- üîç Examine the chunked text data\n",
    "-- Verify that documents were properly split into searchable chunks\n",
    "SELECT file_name, chunk_id, LEFT(chunk_text, 100) as chunk_preview\n",
    "FROM document_chunks\n",
    "ORDER BY file_name, chunk_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfe995-0977-4801-a424-6ebb956ece0e",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "-- üõë Suspend tasks when pipeline testing is complete\n",
    "-- Uncomment the line below to stop the automated pipeline\n",
    "-- ALTER TASK parse_new_documents SUSPEND;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514e56b-7615-4777-896f-311a2c7bc58d",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "## Step 7: Create Cortex Search Service üîç\n",
    "\n",
    "Enable semantic search capabilities by creating a Cortex Search Service that indexes your processed document chunks for intelligent querying and retrieval in **Snowflake Intelligence**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f3945-efb4-45ea-8480-6fb93cbc1ce4",
   "metadata": {
    "name": "cell24"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc000dfb-9687-4839-9365-6e5f9f7c969f",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "-- üîç Create a Cortex Search Service for semantic document search\n",
    "-- This enables AI-powered search across your processed document chunks\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE pdf_search_service\n",
    "ON chunk_text                                    -- Main search column\n",
    "  ATTRIBUTES file_name, processed_timestamp       -- Additional searchable attributes\n",
    "  WAREHOUSE = compute_wh                          -- Compute resources for search\n",
    "  TARGET_LAG = '30 days'                          -- Data refresh frequency\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'  -- AI embedding model\n",
    "AS \n",
    "(\n",
    "\tSELECT\n",
    "\t\tCHUNK_TEXT,\n",
    "\t\tFILE_NAME,\n",
    "\t\tPROCESSED_TIMESTAMP\n",
    "\tFROM ADVANCED_ANALYTICS.UNSTRUCTURED.DOCUMENT_CHUNKS\n",
    ");\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab5373-3df4-4b90-953f-d98cd78c0ba8",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": "### Next Step: ü§ñ Agent Integration\nAdd this cortex search service to a **Cortex Agent** and start having intelligent conversations with your documents in **Snowflake Intelligence**!"
  },
  {
   "cell_type": "markdown",
   "id": "a2eda572-4c45-4b96-83e8-3c987775b993",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## üéâ Complete AI-Powered Document Pipeline!\n",
    "\n",
    "Your end-to-end document processing and search pipeline is now fully operational and ready to automatically:\n",
    "\n",
    "1. **Monitor** üëÄ uploaded PDF files in the stage\n",
    "2. **Parse** üìÑ documents using Snowflake's AI_PARSE_DOCUMENT function  \n",
    "3. **Chunk** ‚úÇÔ∏è text into searchable segments using recursive character splitting\n",
    "4. **Store** üíæ processed data in organized tables\n",
    "5. **Index** üîç content with Cortex Search Service for semantic search\n",
    "\n",
    "### Next Steps:\n",
    "- Upload PDF files to your `pdfs_stage` and watch the automation work! \n",
    "- Monitor the pipeline using the verification queries above\n",
    "- **Connect to Snowflake Intelligence** to chat with your documents using natural language\n",
    "- Scale the pipeline by adjusting warehouse sizes and task schedules\n",
    "- Explore advanced search queries using the Cortex Search Service\n"
   ]
  }
 ]
}