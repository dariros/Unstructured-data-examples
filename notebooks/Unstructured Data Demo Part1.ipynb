{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "426qzc7u777442ckaony",
   "authorId": "48480042761",
   "authorName": "DARIA",
   "authorEmail": "daria.rostovtseva@snowflake.com",
   "sessionId": "c9858a30-b93a-4658-8b75-50fba40fc2a1",
   "lastEditTime": 1760732648143
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da696aa-48bc-4116-8c8e-4dab57816046",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "# Unstructured Data in Snowflake 101\n",
    "\n",
    "## Demo Overview\n",
    "This notebook demonstrates how to work with unstructured data in Snowflake, primarily focusing on text-based documents. It covers:\n",
    "- Creating and configuring stages for file storage\n",
    "- Accessing files through different URL methods\n",
    "- Analyzing unstructured data with SQL AI functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373b901-142b-4f7e-9097-23d4e9552eda",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## 1. Understanding Snowflake Stages\n",
    "\n",
    "To leverage unstructured files in Snowflake, you can store them in an **internal** or **external** stage:\n",
    "\n",
    "- **External stages**: Pointers to cloud storage (AWS S3, Azure Storage, Google Cloud Storage)\n",
    "- **Internal stages**: Native Snowflake storage with built-in encryption\n",
    "\n",
    "### Creating an External Stage\n",
    "```sql\n",
    "-- Generic syntax for external stage\n",
    "CREATE STAGE mydb.myschema.mystage_ext\n",
    "  URL='s3://load/files/'\n",
    "  STORAGE_INTEGRATION = my_storage_int\n",
    "  DIRECTORY = (\n",
    "    ENABLE = true\n",
    "    AUTO_REFRESH = true\n",
    "  )\n",
    "  COMMENT = 'External (S3) Snowflake stage for permits data';\n",
    "```\n",
    "\n",
    "### Creating an Internal Stage\n",
    "```sql\n",
    "-- Generic syntax for internal stage\n",
    "CREATE STAGE  mydb.myschema.mystage_int \n",
    "  DIRECTORY = (\n",
    "    ENABLE = true\n",
    "    AUTO_REFRESH = true\n",
    "   )\n",
    "  ENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' ) \n",
    "  COMMENT = 'Internal Snowflake stage for permits data';\n",
    "```\n",
    "\n",
    "> **üí° Pro Tip:** Always enable the directory table - it acts as a metadata catalog for files in the stage and is essential for automated processing.\n",
    "\n",
    "> **‚ö†Ô∏è Note:** Auto-refresh on internal stages is currently supported for accounts on AWS. Support for accounts on Azure is expected soon.\n",
    "\n",
    "### üìö Documentation References\n",
    "- [Create external stage](https://docs.snowflake.com/en/user-guide/data-load-s3-create-stage)\n",
    "- [Configure external stage to auto-refresh](https://docs.snowflake.com/en/user-guide/data-load-dirtables-auto-s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77409cf2-2328-424b-8763-bb5bf9731da2",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "## 2. Example: External S3 Stage Configuration\n",
    "\n",
    "Let's examine a real-world example of an external S3 stage. Key features to note:\n",
    "\n",
    "- **üìç Location**: Points to a specific partition in an S3 bucket\n",
    "- **üîê Authentication**: Uses a storage integration object for secure AWS access  \n",
    "- **üîÑ Auto-refresh**: Automatically detects new files for continuous processing pipelines\n",
    "\n",
    "The following query shows the configuration of our demo stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146a2d3-a7cf-465f-8a15-5a2ffc3c77cb",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "sql",
    "name": "stage_content_reset"
   },
   "outputs": [],
   "source": [
    "--reset if necessary\n",
    "rm @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3/c4;\n",
    "rm @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3/c5;\n",
    "ALTER STAGE ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3 REFRESH;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "USE ROLE SYSADMIN;\n",
    "-- Describe the stage configuration and filter for key properties\n",
    "DESC STAGE ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3;\n",
    "\n",
    "-- Show only the most important configuration details\n",
    "SELECT * \n",
    "FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) \n",
    "WHERE \"parent_property\" IN ('STAGE_LOCATION','STAGE_INTEGRATION','DIRECTORY');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca97bbb-8b10-43e0-8157-02bf5937905a",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "## 3. Exploring Stage Contents\n",
    "\n",
    "Once your stage is configured, you can explore its contents using two primary methods:\n",
    "\n",
    "### Method 1: List the Stage\n",
    "- Shows file metadata including size, last modified date, and MD5 hash\n",
    "- Good for getting basic file information\n",
    "\n",
    "### Method 2: Query the Directory Table  \n",
    "- Provides structured metadata that can be joined with other tables\n",
    "- Better for automated processing and analytics workflows\n",
    "\n",
    "> **‚ö†Ô∏è Troubleshooting**: If you don't see expected files, ensure auto-refresh is enabled or manually refresh the stage:\n",
    "> ```sql\n",
    "> ALTER STAGE DB_NAME.SCHEMA_NAME.STAGE_NAME REFRESH;\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "-- Method 1: List all files in the stage with metadata\n",
    "LIST @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f85e19-0641-47ca-890b-70a6365ea6f2",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "-- Method 2: Query the directory table for structured metadata\n",
    "SELECT \n",
    "*\n",
    "FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "ORDER BY relative_path;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b54706-0e84-4095-90da-8fa5ec73abe9",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## 4. File Access Methods in Snowflake\n",
    "\n",
    "Snowflake provides three distinct URL-based methods for accessing files in cloud storage, each designed for different use cases:\n",
    "\n",
    "### üîê 1. Scoped UR: BUILD_SCOPED_FILE_URL()\n",
    "- **Security**: Role-based access control\n",
    "- **Duration**: 24 hours (temporary)\n",
    "- **Authentication**: Requires role privileges on view (not stage directly)\n",
    "- **Features**: Provides audit trail in query history\n",
    "- **Best for**: Controlled access within same account, data sharing, Snowsight analysis\n",
    "\n",
    "### üîó 2. File URL: BUILD_STAGE_FILE_URL()\n",
    "- **Security**: Token-based authentication\n",
    "- **Duration**: Permanent URL\n",
    "- **Authentication**: Requires sufficient stage privileges + authorization token\n",
    "- **Features**: Works with REST API GET requests\n",
    "- **Best for**: Custom applications needing persistent file access\n",
    "\n",
    "### üåê 3. Pre-signed URL: GET_PRESIGNED_URL()\n",
    "- **Security**: Open access (no Snowflake authentication required)\n",
    "- **Duration**: Configurable expiration time\n",
    "- **Authentication**: None required - completely public access\n",
    "- **Features**: Simple HTTPS URL\n",
    "- **Best for**: BI tools, reporting applications, public file sharing\n",
    "\n",
    "### üìä Security Comparison\n",
    "| Method | Security Level | Duration | Authentication Required |\n",
    "|--------|---------------|----------|------------------------|\n",
    "| Scoped URL | Highest (Role-based) | 24 hours | Role privileges |\n",
    "| File URL | Medium (Token-based) | Permanent | Stage privileges + token |\n",
    "| Pre-signed URL | Lowest (Open access) | Configurable | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d755f-29df-493c-9ffb-3c8314d07d9c",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "-- Example: Generate a pre-signed URL for a specific file\n",
    "-- This URL will be valid for 3600 seconds (1 hour)\n",
    "SELECT GET_PRESIGNED_URL(\n",
    "    @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3, \n",
    "    'c1.pdf', \n",
    "    3600\n",
    ") as presigned_url;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16c57d-2665-4a0f-832f-f538311341e6",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "-- Example: Generate a pre-signeds URL for all files\n",
    "-- This URL will be valid for 3600 seconds (1 hour)\n",
    "SELECT \n",
    "relative_path filename,\n",
    "GET_PRESIGNED_URL(\n",
    "    @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3, \n",
    "    relative_path, \n",
    "    3600\n",
    ") as presigned_url\n",
    "FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "ORDER BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c807a9-6714-4438-82af-836544e2f781",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "## 5. Interactive PDF Viewer with Streamlit\n",
    "\n",
    "Now let's build an interactive Streamlit application to view and explore the PDF files stored in our Snowflake stage. This demonstrates how to create user-friendly interfaces for unstructured data analysis.\n",
    "\n",
    "**Note:** The pypdfium2 package should be added to your notebook packages for proper PDF rendering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0aa78-8704-4b72-a8f8-4f9d406d6ab3",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pypdfium2 as pdfium\n",
    "import os\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Get the active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "# Set up the Streamlit interface\n",
    "st.title(\"üìÑ PDF Document Explorer\")\n",
    "st.markdown(\"Explore PDF documents stored in your Snowflake stage\")\n",
    "\n",
    "# Create two columns for better layout\n",
    "col1, col2 = st.columns([1, 2])\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"üìÇ Available Documents\")\n",
    "    \n",
    "    # Query the stage directory to get all PDF files\n",
    "    files_query = \"\"\"\n",
    "    SELECT \n",
    "        relative_path,\n",
    "        size,\n",
    "        last_modified,\n",
    "        file_url\n",
    "    FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "    WHERE relative_path LIKE '%.pdf'\n",
    "    ORDER BY last_modified DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query and get results\n",
    "    files_df = session.sql(files_query).to_pandas()\n",
    "    \n",
    "    if not files_df.empty:\n",
    "        # Handle uppercase column names from Snowflake\n",
    "        display_columns = []\n",
    "        \n",
    "        for col in files_df.columns:\n",
    "            if col.upper() in ['RELATIVE_PATH', 'SIZE', 'LAST_MODIFIED']:\n",
    "                display_columns.append(col)\n",
    "        \n",
    "        # Display file information using actual column names\n",
    "        if display_columns:\n",
    "            st.dataframe(\n",
    "                files_df[display_columns], \n",
    "                use_container_width=True,\n",
    "                hide_index=True\n",
    "            )\n",
    "        else:\n",
    "            st.dataframe(files_df, use_container_width=True, hide_index=True)\n",
    "        \n",
    "        # Get the correct column name for relative_path\n",
    "        relative_path_col = None\n",
    "        for col in files_df.columns:\n",
    "            if col.upper() == 'RELATIVE_PATH':\n",
    "                relative_path_col = col\n",
    "                break\n",
    "        \n",
    "        if relative_path_col:\n",
    "            # File selector\n",
    "            selected_file = st.selectbox(\n",
    "                \"Select a PDF to view:\",\n",
    "                files_df[relative_path_col].tolist(),\n",
    "                key=\"pdf_selector\"\n",
    "            )\n",
    "            \n",
    "            # Show file details\n",
    "            if selected_file:\n",
    "                selected_row = files_df[files_df[relative_path_col] == selected_file].iloc[0]\n",
    "                st.markdown(\"**File Details:**\")\n",
    "                \n",
    "                # Handle different possible column names\n",
    "                size_col = next((col for col in files_df.columns if col.upper() == 'SIZE'), None)\n",
    "                modified_col = next((col for col in files_df.columns if col.upper() == 'LAST_MODIFIED'), None)\n",
    "                \n",
    "                if size_col:\n",
    "                    st.write(f\"‚Ä¢ **Size:** {selected_row[size_col]:,} bytes\")\n",
    "                if modified_col:\n",
    "                    st.write(f\"‚Ä¢ **Last Modified:** {selected_row[modified_col]}\")\n",
    "        else:\n",
    "            st.error(\"Could not find relative_path column in the data\")\n",
    "            \n",
    "    else:\n",
    "        st.warning(\"No PDF files found in the stage.\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"üìñ Document Viewer\")\n",
    "    \n",
    "    if not files_df.empty and 'selected_file' in locals() and selected_file:\n",
    "        try:\n",
    "            # Download the PDF file locally for rendering\n",
    "            stage_path = f\"@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3/{selected_file}\"\n",
    "            temp_dir = \"/tmp\"\n",
    "            \n",
    "            # Download file to local temp directory\n",
    "            with st.spinner(f\"Loading {selected_file}...\"):\n",
    "                session.file.get(stage_path, temp_dir)\n",
    "                local_file_path = os.path.join(temp_dir, os.path.basename(selected_file))\n",
    "            \n",
    "            # Render PDF as image using pypdfium2\n",
    "            if os.path.exists(local_file_path):\n",
    "                st.markdown(\"### PDF Preview\")\n",
    "                \n",
    "                # Generate pre-signed URL for download\n",
    "                presigned_query = f\"\"\"\n",
    "                SELECT GET_PRESIGNED_URL(\n",
    "                    @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3, \n",
    "                    '{selected_file}', \n",
    "                    3600\n",
    "                ) as presigned_url\n",
    "                \"\"\"\n",
    "                \n",
    "                url_result = session.sql(presigned_query).collect()\n",
    "                if url_result:\n",
    "                    presigned_url = url_result[0]['PRESIGNED_URL']\n",
    "                    st.markdown(f\"[üì• Download PDF]({presigned_url})\")\n",
    "                \n",
    "                # Render PDF pages as images\n",
    "                pdf_document = pdfium.PdfDocument(local_file_path)\n",
    "                \n",
    "                # Show first page by default, with option to navigate\n",
    "                total_pages = len(pdf_document)\n",
    "                \n",
    "                if total_pages > 1:\n",
    "                    page_num = st.slider(\n",
    "                        f\"Select page (1 to {total_pages}):\",\n",
    "                        min_value=1,\n",
    "                        max_value=total_pages,\n",
    "                        value=1\n",
    "                    ) - 1  # Convert to 0-based index\n",
    "                else:\n",
    "                    page_num = 0\n",
    "                \n",
    "                # Render the selected page\n",
    "                page = pdf_document[page_num]\n",
    "                pil_image = page.render(scale=2.0).to_pil()  # Higher scale for better quality\n",
    "                \n",
    "                st.image(\n",
    "                    pil_image, \n",
    "                    caption=f\"{selected_file} - Page {page_num + 1} of {total_pages}\",\n",
    "                    use_column_width=True\n",
    "                )\n",
    "                \n",
    "                # Additional file operations\n",
    "                st.markdown(\"### üõ†Ô∏è File Operations\")\n",
    "                \n",
    "                col_a, col_b = st.columns(2)\n",
    "                \n",
    "                with col_a:\n",
    "                    if st.button(\"üîÑ Refresh File List\"):\n",
    "                        st.rerun()\n",
    "                \n",
    "                with col_b:\n",
    "                    if st.button(\"üîç View All Pages\"):\n",
    "                        st.markdown(\"**All Pages Preview:**\")\n",
    "                        for i in range(min(total_pages, 3)):  # Show first 3 pages max\n",
    "                            page = pdf_document[i]\n",
    "                            pil_image = page.render(scale=1.0).to_pil()\n",
    "                            st.image(\n",
    "                                pil_image, \n",
    "                                caption=f\"Page {i + 1}\",\n",
    "                                width=300\n",
    "                            )\n",
    "                            if i >= 2 and total_pages > 3:\n",
    "                                st.write(f\"... and {total_pages - 3} more pages\")\n",
    "                                break\n",
    "                \n",
    "                # Clean up\n",
    "                pdf_document.close()\n",
    "                \n",
    "                # Optional: Remove temp file\n",
    "                try:\n",
    "                    os.remove(local_file_path)\n",
    "                except:\n",
    "                    pass  # Ignore cleanup errors\n",
    "                    \n",
    "            else:\n",
    "                st.error(\"Failed to download PDF file\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error loading PDF: {str(e)}\")\n",
    "            st.exception(e)\n",
    "    else:\n",
    "        st.info(\"üëà Select a PDF file from the list to view it here.\")\n",
    "\n",
    "# Add some metrics at the bottom\n",
    "if not files_df.empty:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"üìä Stage Statistics\")\n",
    "    \n",
    "    metrics_col1, metrics_col2, metrics_col3 = st.columns(3)\n",
    "    \n",
    "    # Find the size column (handle case variations)\n",
    "    size_col = next((col for col in files_df.columns if col.upper() == 'SIZE'), None)\n",
    "    \n",
    "    with metrics_col1:\n",
    "        st.metric(\"Total PDF Files\", len(files_df))\n",
    "    \n",
    "    with metrics_col2:\n",
    "        if size_col and files_df[size_col].dtype in ['int64', 'float64']:\n",
    "            total_size_mb = files_df[size_col].sum() / (1024 * 1024)\n",
    "            st.metric(\"Total Size\", f\"{total_size_mb:.1f} MB\")\n",
    "        else:\n",
    "            st.metric(\"Total Size\", \"N/A\")\n",
    "    \n",
    "    with metrics_col3:\n",
    "        if size_col and files_df[size_col].dtype in ['int64', 'float64']:\n",
    "            avg_size_kb = files_df[size_col].mean() / 1024\n",
    "            st.metric(\"Average File Size\", f\"{avg_size_kb:.1f} KB\")\n",
    "        else:\n",
    "            st.metric(\"Average File Size\", \"N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719cdb2-c8c8-4bce-affa-180ce471dd5a",
   "metadata": {
    "collapsed": false,
    "name": "cell13"
   },
   "source": [
    "## 6. AI-Powered Document Classification with [AI_EXTRACT](https://docs.snowflake.com/en/sql-reference/functions/ai_extract)\n",
    "\n",
    "Now let's see how we can apply one of Snowflake's powerful features for unstructured data analysis: **AI_EXTRACT function**. This function can automatically analyze documents and extract structured information using advanced AI models. It uses a zero-shot approach at classification and information extraction. For few-shot classification, you can use [AI_CLASSIFY](https://docs.snowflake.com/en/sql-reference/functions/ai_classify). For more complex extraction, such as extracting tables from images/pdfs and training models on complex documents, use [Document AI](https://docs.snowflake.com/en/user-guide/snowflake-cortex/document-ai/overview) (more on that later).\n",
    "\n",
    "AI_EXTRACT Key capabilities:\n",
    "\n",
    "* Extracts data from PDFs, images, Word docs, emails, and more\n",
    "* Works with multiple languages\n",
    "* Lets you define exactly what information to pull out\n",
    "* Processes files directly without moving data around\n",
    "* Handles complex extraction tasks in a single operation\n",
    "\n",
    "Syntax examples: \n",
    "```sql\n",
    "--extract info from text\n",
    "SELECT AI_EXTRACT(\n",
    " text => 'John Smith lives in San Francisco and works for Snowflake',\n",
    " responseFormat => {'name': 'What is the first name of the employee?', 'city': 'What is the address of the employee?'}\n",
    ");\n",
    "```\n",
    "\n",
    "```sql\n",
    "--extract info from file\n",
    "SELECT AI_EXTRACT(\n",
    "  file => TO_FILE('@db.schema.files','document.pdf'),\n",
    "  responseFormat => [['name', 'What is the first name of the employee?'], ['city', 'Where does the employee live?']]\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c4a68-1d22-4c20-ac9a-4677abf0d1ba",
   "metadata": {
    "language": "sql",
    "name": "cell14",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Example 1: Document Classification\n",
    "-- Let's classify what type of document we're looking at\n",
    "SELECT \n",
    "    'c1.pdf' as filename,\n",
    "    AI_EXTRACT(\n",
    "        file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', 'c1.pdf'),\n",
    "        responseFormat => [[\n",
    "            'document_type', \n",
    "                'What type of document is this? Choose from: \n",
    "                permit_application, \n",
    "                contractor_license, \n",
    "                invoice, \n",
    "                contract, \n",
    "                inspection_report, \n",
    "                or other'\n",
    "        ]]\n",
    "    ) as classification_result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03ed49-3183-4272-b07e-6defd4fc1de4",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Example 2: Extract Key Information from multiple documents\n",
    "-- Now let's extract specific data fields from the document\n",
    "\n",
    "SELECT \n",
    "    relative_path as filename,\n",
    "    AI_EXTRACT(\n",
    "        file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', relative_path),\n",
    "        responseFormat => [\n",
    "            ['document_title', 'What is the title or header of this document?'],\n",
    "            ['applicant_name', 'Who is the applicant or main person named in this document?'],\n",
    "            ['company_name', 'What is the company or business name mentioned?'],\n",
    "            ['phone_number', 'What phone number is provided?'],\n",
    "            ['email_address', 'What email address is listed?'],\n",
    "            ['address', 'What is the main address mentioned in the document?'],\n",
    "            ['date', 'What date is prominently displayed on the document?'],\n",
    "            ['has_signature', 'Is there a signature present on this document? (true/false)'],\n",
    "            ['document_summary', 'Provide a brief 1-2 sentence summary of what this document is about']\n",
    "        ]\n",
    "    ) as extracted_data\n",
    "    from directory(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "    limit 3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca44de4-011e-46f2-942e-e9af1006bd28",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "-- Example 3: Process Multiple Documents with Classification and Extraction\n",
    "-- This query classifies multiple documents and extracts basic info from each\n",
    "\n",
    "SELECT \n",
    "    relative_path filename,\n",
    "    AI_EXTRACT(\n",
    "        file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', relative_path),\n",
    "        responseFormat => [\n",
    "            ['document_type', \n",
    "                'What type of document is this? Choose from: \n",
    "                permit_application, \n",
    "                contractor_license, \n",
    "                invoice, \n",
    "                contract, \n",
    "                inspection_report, \n",
    "                or other'\n",
    "            ],\n",
    "            ['main_person', 'Who is the main person or applicant mentioned?'],\n",
    "            ['organization', 'What company or organization is involved?'],\n",
    "            ['key_date', 'What is the most important date mentioned?'],\n",
    "            ['confidence', 'How confident are you in the document type classification? (high/medium/low)']\n",
    "        ]\n",
    "    ) as extracted_info_json,\n",
    "    extracted_info_json:response.document_type::string document_type,\n",
    "    extracted_info_json:response.key_date::date key_date,\n",
    "    extracted_info_json:response.main_person::string main_person,\n",
    "    extracted_info_json:response.organization::string organization,\n",
    "FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "WHERE filename LIKE '%.pdf'  \n",
    "ORDER BY filename\n",
    "LIMIT 3;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b14b84-9aab-4235-92dd-a28e0cb475e7",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "## üîÑ Building Incremental Processing Pipelines\n",
    "\n",
    "### **Change Data Capture for Files**\n",
    "Snowflake provides powerful change data capture capabilities on stages that enable building incremental processing pipelines. This is accomplished by creating **streams** on top of stages.\n",
    "\n",
    "### **How It Works:**\n",
    "üéØ **Step 1**: New files are added to the stage  \n",
    "üìä **Step 2**: Stream automatically detects the changes  \n",
    "‚ö° **Step 3**: Process only the new files (not existing ones)  \n",
    "üîÑ **Step 4**: Stream auto-flushes after data consumption  \n",
    "\n",
    "### **Key Benefits:**\n",
    "- üéØ **Automatic Detection**: Streams automatically track new files added to stages\n",
    "- ‚ö° **Incremental Processing**: Only process new files, not the entire stage\n",
    "- üîÑ **Self-Managing**: Streams automatically flush after data consumption\n",
    "- üìä **Cost Effective**: Avoid reprocessing existing documents\n",
    "\n",
    "### **Implementation Steps:**\n",
    "1. Create a stream on your stage\n",
    "2. Query the stream to see new files\n",
    "3. Process the new files with your logic\n",
    "4. Stream automatically resets for the next batch\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8bc74-1a2e-4e48-b062-ef161fa34e7d",
   "metadata": {
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "--let's look again at the contents of our stage PERMITS_S3\n",
    "SELECT * FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678a61f-87c8-4859-852a-fdc254ac009f",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "--let's do an initial bulk load of document info into a table\n",
    "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.UNSTRUCTURED.PERMIT_INFO \n",
    "AS\n",
    "SELECT \n",
    "    relative_path filename,\n",
    "    AI_EXTRACT(\n",
    "        file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', relative_path),\n",
    "        responseFormat => [\n",
    "            ['document_type', \n",
    "                'What type of document is this? Choose from: \n",
    "                permit_application, \n",
    "                contractor_license, \n",
    "                invoice, \n",
    "                contract, \n",
    "                inspection_report, \n",
    "                or other'\n",
    "            ],\n",
    "            ['main_person', 'Who is the main person or applicant mentioned?'],\n",
    "            ['organization', 'What company or organization is involved?'],\n",
    "            ['key_date', 'What is the most important date mentioned?'],\n",
    "            ['confidence', 'How confident are you in the document type classification? (high/medium/low)']\n",
    "        ]\n",
    "    ) as extracted_info_json,\n",
    "    extracted_info_json:response.document_type::string document_type,\n",
    "    extracted_info_json:response.key_date::date key_date,\n",
    "    extracted_info_json:response.main_person::string main_person,\n",
    "    extracted_info_json:response.organization::string organization,\n",
    "FROM DIRECTORY(@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3)\n",
    "WHERE filename LIKE '%.pdf' ;\n",
    "\n",
    "SELECT COUNT(*) FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMIT_INFO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb109e-d2ef-4cc7-a6cc-439593ad91a6",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "--let's create a new stream on the stage\n",
    "CREATE OR REPLACE STREAM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM ON STAGE ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d13f7-ae07-4330-a735-ee984c3bbb69",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "--initially, the stream is empty\n",
    "SELECT * FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21847c9-dfb0-45d4-8507-5caed0a7b711",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "--lets copy a few files from another stage into the PERMITS_S3 stage\n",
    "--this is simulating the files being dropped into storage\n",
    "COPY FILES INTO @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3 from @ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_SNOWFLAKE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85ddbf-02db-4493-93b5-a8ae278eaded",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "--in a few seconds, Snowflake will receive a notification from S3 about the new files, and the stream will show the new files added to the stage\n",
    "--this typically takes 30-60 seconds\n",
    "--if you want to speed things up, you can run:\n",
    "--ALTER STAGE ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3 REFRESH;\n",
    "\n",
    "SELECT * FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e792039-05eb-422a-b875-3181a4f4778f",
   "metadata": {
    "language": "sql",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "--process the files incrementally from the stream - extract the info and insert into the table\n",
    "INSERT INTO ADVANCED_ANALYTICS.UNSTRUCTURED.PERMIT_INFO \n",
    "SELECT \n",
    "    relative_path filename,\n",
    "    AI_EXTRACT(\n",
    "        file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', relative_path),\n",
    "        responseFormat => [\n",
    "            ['document_type', \n",
    "                'What type of document is this? Choose from: \n",
    "                permit_application, \n",
    "                contractor_license, \n",
    "                invoice, \n",
    "                contract, \n",
    "                inspection_report, \n",
    "                or other'\n",
    "            ],\n",
    "            ['main_person', 'Who is the main person or applicant mentioned?'],\n",
    "            ['organization', 'What company or organization is involved?'],\n",
    "            ['key_date', 'What is the most important date mentioned?'],\n",
    "            ['confidence', 'How confident are you in the document type classification? (high/medium/low)']\n",
    "        ]\n",
    "    ) as extracted_info_json,\n",
    "    extracted_info_json:response.document_type::string document_type,\n",
    "    extracted_info_json:response.key_date::date key_date,\n",
    "    extracted_info_json:response.main_person::string main_person,\n",
    "    extracted_info_json:response.organization::string organization,\n",
    "FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc07d4f-9974-4fbb-a9c0-f2f108427103",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMIT_INFO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc3629-6a0c-4ad2-8870-cf36ff393c84",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "--stream is now flushed\n",
    "SELECT * FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d7b6e-33e6-47b6-9fdb-30dd189a3b0a",
   "metadata": {
    "language": "sql",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "-- ü§ñ Create an automated task that runs the INSERT when stream has data\n",
    "-- This creates a fully automated pipeline that processes new files as they arrive\n",
    "\n",
    "CREATE OR REPLACE TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK\n",
    "    -- Task configuration\n",
    "    -- WAREHOUSE = 'COMPUTE_WH'  -- warehouses are now optional in most cases, use serverless!\n",
    "    SCHEDULE = '5 MINUTE'     -- Check every 5 minutes\n",
    "    -- Condition: Only run when stream has data\n",
    "    WHEN SYSTEM$STREAM_HAS_DATA('ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM')\n",
    "AS\n",
    "    -- The INSERT statement from our previous example\n",
    "    INSERT INTO ADVANCED_ANALYTICS.UNSTRUCTURED.PERMIT_INFO \n",
    "    SELECT \n",
    "        relative_path filename,\n",
    "        AI_EXTRACT(\n",
    "            file => TO_FILE('@ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3', relative_path),\n",
    "            responseFormat => [\n",
    "                ['document_type', \n",
    "                    'What type of document is this? Choose from: \n",
    "                    permit_application, \n",
    "                    contractor_license, \n",
    "                    invoice, \n",
    "                    contract, \n",
    "                    inspection_report, \n",
    "                    or other'\n",
    "                ],\n",
    "                ['main_person', 'Who is the main person or applicant mentioned?'],\n",
    "                ['organization', 'What company or organization is involved?'],\n",
    "                ['key_date', 'What is the most important date mentioned?'],\n",
    "                ['confidence', 'How confident are you in the document type classification? (high/medium/low)']\n",
    "            ]\n",
    "        ) as extracted_info_json,\n",
    "        extracted_info_json:response.document_type::string document_type,\n",
    "        extracted_info_json:response.key_date::date key_date,\n",
    "        extracted_info_json:response.main_person::string main_person,\n",
    "        extracted_info_json:response.organization::string organization\n",
    "    FROM ADVANCED_ANALYTICS.UNSTRUCTURED.PERMITS_S3_STREAM;\n",
    "\n",
    "-- Enable the task to start running\n",
    "ALTER TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK RESUME;\n",
    "\n",
    "--Suspend when no longer needed\n",
    "ALTER TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK SUSPEND;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc4940-76ae-4dc4-9a86-682335b55474",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": [
    "## üìã Task Management & Monitoring\n",
    "\n",
    "### **Key Components Explained:**\n",
    "\n",
    "‚è∞ **SCHEDULE**: How often the task checks for new data (1 MINUTE, 5 MINUTE, 1 HOUR, etc.)  \n",
    "üéØ **WHEN Condition**: `SYSTEM$STREAM_HAS_DATA()` ensures task only runs when there's new data  \n",
    "‚ö° **AUTO-FLUSH**: Stream automatically resets after successful task completion  \n",
    "\n",
    "### **Task Management Commands:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee477da-09da-4d7d-b37e-e2b21b3b1492",
   "metadata": {
    "language": "sql",
    "name": "cell32",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- üîç Monitor task status and execution history\n",
    "SHOW TASKS LIKE 'PROCESS_NEW_PERMITS_TASK';\n",
    "\n",
    "-- üìä View task execution history\n",
    "SELECT *\n",
    "FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n",
    "    TASK_NAME => 'ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK'\n",
    "))\n",
    "ORDER BY SCHEDULED_TIME DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- ‚è∏Ô∏è Suspend the task (stop automatic execution)\n",
    "-- ALTER TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK SUSPEND;\n",
    "\n",
    "-- ‚ñ∂Ô∏è Resume the task (start automatic execution)\n",
    "-- ALTER TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK RESUME;\n",
    "\n",
    "-- üóëÔ∏è Drop the task completely\n",
    "-- DROP TASK ADVANCED_ANALYTICS.UNSTRUCTURED.PROCESS_NEW_PERMITS_TASK;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4e4ef-270d-4b63-9717-61523a949fee",
   "metadata": {
    "name": "cell33"
   },
   "source": [
    "### üéØ **Complete Automation Achieved!**\n",
    "\n",
    "With this task in place, you now have a **fully automated document processing pipeline**:\n",
    "\n",
    "1. **üìÅ Files arrive** ‚Üí S3 stage receives new documents\n",
    "2. **üîÑ Auto-detection** ‚Üí Stream captures new file events  \n",
    "3. **‚ö° Task triggers** ‚Üí Scheduled task checks stream every 5 minutes\n",
    "4. **ü§ñ AI processing** ‚Üí AI_EXTRACT analyzes and extracts data from new files\n",
    "5. **üíæ Data storage** ‚Üí Results automatically inserted into PERMIT_INFO table\n",
    "6. **üîÑ Stream resets** ‚Üí Stream flushes, ready for next batch\n",
    "\n",
    "### üöÄ **Production Benefits:**\n",
    "- **Zero Manual Intervention**: Completely hands-off processing\n",
    "- **Cost Efficient**: Only runs when there's actual work to do  \n",
    "- **Scalable**: Handles hundreds of documents automatically\n",
    "- **Reliable**: Built-in error handling and retry logic\n",
    "- **Auditable**: Full execution history and monitoring capabilities\n",
    "\n",
    "**This is enterprise-grade document automation in action!** üéâ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770fb4e-75f0-498d-9c32-3eb80e74248f",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": [
    "### üí∞ **Understanding AI_EXTRACT Costs**\n",
    "\n",
    "#### **How AI_EXTRACT Billing Works:**\n",
    "Based on the [Snowflake Cortex AISQL documentation](https://docs.snowflake.com/en/user-guide/snowflake-cortex/aisql#cost-considerations), AI_EXTRACT uses Snowflake's **credit-based billing model** for AI services:\n",
    "\n",
    "üî¢ **Token-Based Pricing**: AI_EXTRACT charges based on the number of tokens processed:\n",
    "- **Input tokens**: Text extracted from your documents (files, images, PDFs)\n",
    "- **Output tokens**: The structured JSON response generated by the AI model\n",
    "- **Both input AND output tokens count toward your total cost**\n",
    "\n",
    "üí≥ **Credit Consumption**: \n",
    "- Costs are deducted from your Snowflake credit balance\n",
    "- Different models have different token-to-credit ratios (see [Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf))\n",
    "- Charges appear in metering history under \"AI Services\", for example:\n",
    "\n",
    "#### **Cost Factors That Impact Your Bill:**\n",
    "\n",
    "üìÑ **Document Characteristics:**\n",
    "- **File size**: Larger PDFs = more content = more input tokens\n",
    "- **Text density**: Dense documents cost more than sparse ones\n",
    "- **Image quality**: High-resolution images require more processing\n",
    "- **Document complexity**: Multi-page, multi-column layouts increase token count\n",
    "\n",
    "üéØ **Extraction Scope:**\n",
    "- **Number of fields**: More extraction fields = higher output token count\n",
    "- **Field complexity**: Detailed descriptions require more processing\n",
    "- **Response format**: JSON structure adds to output token count\n",
    "\n",
    "#### **Cost Optimization Strategies:**\n",
    "\n",
    "‚úÖ **Smart Batching**:\n",
    "```sql\n",
    "-- Process multiple documents in single query to amortize overhead\n",
    "SELECT filename, AI_EXTRACT(...) FROM directory_table LIMIT 100;\n",
    "```\n",
    "\n",
    "‚úÖ **Selective Processing**:\n",
    "```sql\n",
    "-- Only process files that haven't been analyzed yet\n",
    "WHERE filename NOT IN (SELECT filename FROM processed_docs);\n",
    "```\n",
    "\n",
    "‚úÖ **Efficient Field Design**:\n",
    "- Use concise field descriptions\n",
    "- Extract only necessary information\n",
    "- Avoid redundant extraction fields\n",
    "\n",
    "#### **Track Your AI_EXTRACT Costs:**\n",
    "```sql\n",
    "-- Monitor credit consumption for AI services\n",
    "SELECT \n",
    "    DATE(start_time) as usage_date,\n",
    "    SUM(credits_used) as total_credits,\n",
    "    COUNT(*) as query_count\n",
    "FROM SNOWFLAKE.ACCOUNT_USAGE.METERING_HISTORY \n",
    "WHERE service_type = 'AI_SERVICES'\n",
    "  AND start_time >= DATEADD('day', -7, CURRENT_DATE())\n",
    "GROUP BY DATE(start_time)\n",
    "ORDER BY usage_date DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c6051-bde5-449f-ae9f-77e786b8915c",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "### üß† **AI_EXTRACT Key Features Demonstrated:**\n",
    "\n",
    "1. **üéØ Document Classification**: Automatically categorize documents by type\n",
    "2. **üìä Structured Data Extraction**: Transform unstructured text into queryable fields\n",
    "3. **üîÑ Batch Processing**: Process multiple documents efficiently\n",
    "4. **üé® Flexible Schemas**: Define custom extraction fields for any document type\n",
    "5. **‚ö° Real-time Processing**: No need for external AI services - everything runs in Snowflake\n",
    "\n",
    "### üí° **Pro Tips:**\n",
    "- **Specific Instructions**: The more specific your field descriptions, the better the extraction quality\n",
    "- **Validation Fields**: Include confidence scores to assess extraction quality\n",
    "- **Incremental Processing**: Use streams and tasks for continuous document processing\n",
    "- **Error Handling**: AI_EXTRACT gracefully handles various document formats and quality levels\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "This demonstrates the foundation for building automated document processing pipelines. You can extend this to:\n",
    "- Create tables to store extracted data permanently\n",
    "- Set up automated workflows with tasks and streams\n",
    "- Build approval workflows for human validation\n",
    "- Create dashboards for document analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe054f-70dd-4618-ad75-71c8f1c6769e",
   "metadata": {
    "collapsed": false,
    "name": "cell28"
   },
   "source": [
    "## 7. Performance & Cost Considerations\n",
    "\n",
    "### üí∞ **Cost Optimization**\n",
    "When working with unstructured data in Snowflake, consider these cost optimization strategies:\n",
    "\n",
    "- **File Size Management**: Larger files take more compute to process with AI_EXTRACT\n",
    "- **Batch Processing**: Process multiple documents in a single query when possible\n",
    "- **Stage Organization**: Use partitioned stage layouts for efficient file access\n",
    "- **Compression**: Store files in compressed formats when possible\n",
    "\n",
    "### üìä **Quality & Performance Tips**\n",
    "\n",
    "#### **Optimal PDF Characteristics:**\n",
    "- **Resolution**: 150-300 DPI for text extraction\n",
    "- **Format**: Text-based PDFs perform better than scanned images\n",
    "- **Size**: Files under 10MB typically process faster\n",
    "- **Language**: Clear, standard fonts improve extraction accuracy\n",
    "\n",
    "#### **Quality Monitoring:**\n",
    "```sql\n",
    "-- Monitor extraction confidence levels\n",
    "SELECT \n",
    "    document_type,\n",
    "    AVG(CASE WHEN confidence = 'high' THEN 3 \n",
    "             WHEN confidence = 'medium' THEN 2 \n",
    "             ELSE 1 END) as avg_confidence_score,\n",
    "    COUNT(*) as document_count\n",
    "FROM PERMIT_INFO\n",
    "GROUP BY document_type\n",
    "ORDER BY avg_confidence_score DESC;\n",
    "```\n",
    "\n",
    "### üéØ **Best Practices Summary**\n",
    "1. **Test extraction prompts** on sample documents first\n",
    "2. **Monitor processing costs** using query profiling\n",
    "3. **Validate results** with confidence scoring\n",
    "4. **Use incremental processing** for ongoing document workflows\n",
    "5. **Optimize file formats** for your specific use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6878b-9024-4f23-a145-c7380e60bb2a",
   "metadata": {
    "name": "cell29"
   },
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### **What You've Learned**\n",
    "This notebook demonstrated the complete workflow for handling unstructured data in Snowflake:\n",
    "\n",
    "‚úÖ **Stage Configuration**: Created and configured external S3 stages with auto-refresh capabilities  \n",
    "‚úÖ **File Access Methods**: Explored scoped URLs, file URLs, and pre-signed URLs for different use cases  \n",
    "‚úÖ **Interactive Visualization**: Built a Streamlit app for document exploration and preview  \n",
    "‚úÖ **AI-Powered Processing**: Used AI_EXTRACT for intelligent document classification and data extraction  \n",
    "‚úÖ **Incremental Pipelines**: Implemented stream-based processing for continuous document workflows  \n",
    "‚úÖ **Performance Optimization**: Learned best practices for cost-effective unstructured data processing  \n",
    "\n",
    "### **üöÄ Ready for Production?**\n",
    "You now have the foundation to build enterprise-scale unstructured data solutions:\n",
    "\n",
    "**For Document Management Systems:**\n",
    "- Automated classification and metadata extraction\n",
    "- Searchable document repositories\n",
    "- Compliance and audit workflows\n",
    "\n",
    "**For Business Process Automation:**\n",
    "- Invoice and contract processing\n",
    "- Permit and application workflows  \n",
    "- Customer document onboarding\n",
    "\n",
    "**For Analytics & Insights:**\n",
    "- Document sentiment analysis\n",
    "- Content trend identification\n",
    "- Regulatory compliance monitoring\n",
    "\n",
    "### **üìö Additional Resources**\n",
    "- [Snowflake Unstructured Data Guide](https://docs.snowflake.com/en/user-guide/unstructured-intro)\n",
    "- [AI_EXTRACT Documentation](https://docs.snowflake.com/en/sql-reference/functions/ai_extract)\n",
    "- [Document AI for Complex Extraction](https://docs.snowflake.com/en/user-guide/snowflake-cortex/document-ai/overview)\n",
    "- [Snowflake Cortex AI Functions](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions)\n",
    "\n",
    "---\n",
    "*Happy analyzing! üéØ*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4ae5d-e0af-40ac-abc2-fdb375505ea3",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "## üéÅ Bonus: Additional Unstructured Data Capabilities\n\n### üìÑ **AI_PARSE_DOCUMENT: Parse All Text**\n\n**Purpose**: Extract text and layout information from complex documents including PDFs, images, and scanned documents using OCR technology.\n\n**Key Features**:\n- **OCR Mode**: Extract raw text from scanned documents and images\n- **LAYOUT Mode**: Preserve document structure, tables, and formatting\n- **Multi-format Support**: Works with PDFs, images (PNG, JPG), and more\n- **Batch Processing**: Process multiple documents efficiently\n\n**Example Use Cases**:\n```sql\n-- Extract all text from a scanned document\nSELECT AI_PARSE_DOCUMENT(\n    @my_stage, \n    'scanned_contract.pdf', \n    {'mode': 'OCR'}\n) as extracted_text;\n\n-- Preserve document layout and structure\nSELECT AI_PARSE_DOCUMENT(\n    @my_stage, \n    'financial_report.pdf', \n    {'mode': 'LAYOUT'}\n) as structured_content;\n```\n\n### üé§ **AI_TRANSCRIBE: Speech-to-Text Processing**\n\n**Purpose**: Convert audio files into text with speaker identification and timestamps.\n\n**Key Features**:\n- **Multi-format Audio**: Supports MP3, WAV, M4A, and other common formats\n- **Speaker Diarization**: Identifies different speakers in conversations\n- **Timestamps**: Provides precise timing for each spoken segment\n- **Language Detection**: Automatically detects spoken language\n\n**Example Use Cases**:\n```sql\n-- Transcribe customer service calls\nSELECT AI_TRANSCRIBE(@call_recordings_stage, 'customer_call.mp3') as transcript;\n\n-- Process meeting recordings with speaker identification\nSELECT \n    filename,\n    AI_TRANSCRIBE(@meeting_stage, filename) as meeting_transcript\nFROM DIRECTORY(@meeting_stage) \nWHERE filename LIKE '%.mp3';\n```\n\n**Business Applications**:\n- Customer service call analysis\n- Meeting transcription and summarization\n- Compliance and audit documentation\n- Voice-to-text data entry automation\n\n### üñºÔ∏è **AI_COMPLETE Multimodal: Advanced Image Analysis**\n\n**Purpose**: Analyze images using large language models for object detection, image comparison, and visual content understanding.\n\n**Key Capabilities**:\n- **Object Detection**: Identify and describe objects, people, text in images\n- **Image Comparison**: Compare multiple images for similarities/differences\n- **Scene Analysis**: Understand context and activities in visual content\n- **OCR Integration**: Extract and interpret text within images\n\n**Example Use Cases**:\n```sql\n-- Analyze product images for inventory management\nSELECT AI_COMPLETE(\n    'llama3.1-70b',\n    TO_FILE(@product_images, 'product_001.jpg'),\n    'Describe this product in detail. What is it? What condition is it in? List any visible defects.'\n) as product_analysis;\n\n-- Compare before/after images for damage assessment\nSELECT AI_COMPLETE(\n    'claude-3-haiku',\n    [TO_FILE(@damage_photos, 'before.jpg'), TO_FILE(@damage_photos, 'after.jpg')],\n    'Compare these two images. What changes or damage do you observe between the before and after photos?'\n) as damage_assessment;\n```\n\n**Business Applications**:\n- Insurance claim processing with photo analysis\n- Quality control in manufacturing\n- Medical imaging analysis and reporting\n- Real estate property assessment\n\n### üîç **AI_SIMILARITY: Advanced Similarity Search**\n\n**Purpose**: Compute similarity scores between texts, images, or mixed content using vector embeddings.\n\n**Key Features**:\n- **Multi-modal Support**: Compare text-to-text, image-to-image, or text-to-image\n- **Vector Embeddings**: Uses advanced embedding models for accurate similarity\n- **Batch Comparisons**: Efficiently compare multiple items at once\n- **Cosine Similarity**: Industry-standard similarity scoring\n\n**Example Use Cases**:\n```sql\n-- Find similar documents in your corpus\nSELECT \n    doc1.filename,\n    doc2.filename,\n    AI_SIMILARITY(doc1.content, doc2.content) as similarity_score\nFROM documents doc1 \nCROSS JOIN documents doc2\nWHERE doc1.filename != doc2.filename\n  AND AI_SIMILARITY(doc1.content, doc2.content) > 0.8;\n\n-- Image similarity for duplicate detection\nSELECT \n    image_name,\n    AI_SIMILARITY(\n        TO_FILE(@image_stage, image_name),\n        TO_FILE(@reference_stage, 'reference_image.jpg')\n    ) as similarity_score\nFROM DIRECTORY(@image_stage)\nWHERE similarity_score > 0.9;\n```\n\n**Business Applications**:\n- Duplicate content detection\n- Recommendation systems  \n- Fraud detection through pattern matching\n- Content organization and categorization\n\n### üöÄ **Complete Workflow Example**\n\nHere's how you might combine these capabilities in a comprehensive document processing pipeline:\n\n```sql\n-- Comprehensive document processing workflow\nWITH document_processing AS (\n    -- Step 1: Parse document structure\n    SELECT \n        filename,\n        AI_PARSE_DOCUMENT(@docs_stage, filename, {'mode': 'LAYOUT'}) as parsed_content\n    FROM DIRECTORY(@docs_stage)\n    WHERE filename LIKE '%.pdf'\n),\nextracted_data AS (\n    -- Step 2: Extract structured information\n    SELECT \n        filename,\n        parsed_content,\n        AI_EXTRACT(\n            parsed_content,\n            [\n                ['document_type', 'What type of document is this?'],\n                ['key_entities', 'List the main entities (people, companies, dates)'],\n                ['summary', 'Provide a brief summary']\n            ]\n        ) as extracted_info\n    FROM document_processing\n),\nsimilarity_analysis AS (\n    -- Step 3: Find similar documents\n    SELECT \n        d1.filename,\n        d1.extracted_info,\n        ARRAY_AGG(d2.filename) as similar_documents\n    FROM extracted_data d1\n    JOIN extracted_data d2 \n        ON AI_SIMILARITY(d1.parsed_content, d2.parsed_content) > 0.8\n        AND d1.filename != d2.filename\n    GROUP BY d1.filename, d1.extracted_info\n)\nSELECT * FROM similarity_analysis;\n```\n\n### üìö **Additional Resources**\n\n- [AI_PARSE_DOCUMENT Documentation](https://docs.snowflake.com/en/sql-reference/functions/ai_parse_document)\n- [AI_TRANSCRIBE Documentation](https://docs.snowflake.com/en/sql-reference/functions/ai_transcribe)  \n- [AI_COMPLETE Multimodal Guide](https://docs.snowflake.com/en/user-guide/snowflake-cortex/aisql)\n- [AI_SIMILARITY Documentation](https://docs.snowflake.com/en/sql-reference/functions/ai_similarity)\n\n---\n**üéØ These advanced capabilities transform Snowflake into a complete unstructured data platform, enabling sophisticated AI-powered analysis across text, audio, images, and documents!**\n"
  }
 ]
}